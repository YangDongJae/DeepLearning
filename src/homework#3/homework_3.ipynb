{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qME02Vhk_1cn",
    "outputId": "da6d3014-eb58-4349-d3eb-e8987e297989"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "rZizIV6tARnq"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/linklab/link_dl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "mEKzB3OI-I9l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.nn.functional import softmax\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "jqqybZBF_sfq"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"C:/Users/nazero/Desktop/link_dl-main/link_dl-main\"\n",
    "sys.path.append(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "93Nxp_B2AC2z"
   },
   "outputs": [],
   "source": [
    "from _01_code._99_common_utils.utils import get_num_cpu_cores, is_linux, is_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Approach Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "wjFgqb0hGRl7"
   },
   "outputs": [],
   "source": [
    "def get_mean_std(imgs):\n",
    "  mean = imgs.view(1, -1).mean(dim = -1)\n",
    "  std = imgs.view(1, -1).std(dim = -1)\n",
    "\n",
    "  return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "AAyFE8IqAo6N"
   },
   "outputs": [],
   "source": [
    "def get_fashion_mnist_data():\n",
    "  data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "  f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "  f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "  print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "  print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "  print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "  num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "  print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=f_mnist_train,\n",
    "     batch_size=wandb.config.batch_size,\n",
    "     shuffle=True,\n",
    "    pin_memory=True,\n",
    "     num_workers=num_data_loading_workers,\n",
    "     drop_last = True\n",
    "  )\n",
    "\n",
    "  validation_data_loader = DataLoader(\n",
    "    dataset=f_mnist_validation,\n",
    "     batch_size=wandb.config.batch_size,\n",
    "     pin_memory=True,\n",
    "     num_workers=num_data_loading_workers,\n",
    "     drop_last = True\n",
    "  )\n",
    "\n",
    "  imgs = torch.stack([img_t for img_t, _ in train_data_loader], dim = 3)\n",
    "  x, y = get_mean_std(imgs)\n",
    "\n",
    "  f_mnist_transforms = nn.Sequential(\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(mean=x, std=y),\n",
    "  )\n",
    "\n",
    "  return train_data_loader, validation_data_loader, f_mnist_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ogBODywhBVYP"
   },
   "outputs": [],
   "source": [
    "def get_fashion_mnist_test_data():\n",
    "  data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "  f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "  f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "  print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "  print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "  test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "  imgs = torch.stack([img_t for img_t, _ in test_data_loader], dim = 3)\n",
    "  x, y = get_mean_std(imgs)\n",
    "\n",
    "  f_mnist_transforms = nn.Sequential(\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(mean=x, std=y),\n",
    "  )\n",
    "\n",
    "  return f_mnist_test_images, test_data_loader, f_mnist_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "hNIPgHAgBZAZ",
    "outputId": "c6f45a1a-ef9c-43e3-b412-a6fdadcb3508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 32\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "config = {'batch_size': 2048,}\n",
    "wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "print()\n",
    "f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 996
    },
    "id": "fF74SzeAbYr3",
    "outputId": "79774a08-013c-4db7-b715-42e45e897254"
   },
   "outputs": [],
   "source": [
    "# pil_image = f_mnist_test_images[0][0]\n",
    "# image_tensor = to_tensor(pil_image)\n",
    "# normalized_image_tensor = f_mnist_transforms(image_tensor)\n",
    "\n",
    "# def show_image(img_tensor, title=''):\n",
    "#     # If image has been normalized, denormalize it for visualization\n",
    "#     if img_tensor.ndimension() == 3:\n",
    "#         img_tensor = img_tensor.squeeze(0)  # Remove channel dimension if it's a single image\n",
    "\n",
    "#     np_img = img_tensor.numpy()\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(np_img, cmap='gray')\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "\n",
    "# show_image(image_tensor, title='Original Image')\n",
    "# show_image(normalized_image_tensor, title='Normalized Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "vIFdhnHCcfuQ",
    "outputId": "761efad2-c99b-4108-a880-b816929fa674"
   },
   "outputs": [],
   "source": [
    "# # Function to plot the histogram\n",
    "# def plot_histogram(img_tensor, title):\n",
    "#     np_img = img_tensor.numpy().flatten()\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.hist(np_img, bins=50)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Pixel Intensity')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# # Plot histogram for the original and normalized images\n",
    "# plot_histogram(image_tensor, 'Histogram of Original Image')\n",
    "# plot_histogram(normalized_image_tensor, 'Histogram of Normalized Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "Cu4Y_XxVS_op",
    "outputId": "ce97ea1d-b612-4edc-a75c-b50c47ee5d1c"
   },
   "outputs": [],
   "source": [
    "# for i in range (9):\n",
    "#   plt.subplot(330 + 1 + i)\n",
    "#   plt.imshow(f_mnist_test_images[i][0], cmap = plt.get_cmap('gray'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "t3uKV7jbS8Ro",
    "outputId": "b58307ec-107f-4ed3-fb24-1187f2e3d7a1"
   },
   "outputs": [],
   "source": [
    "# label_count = [0] * 10\n",
    "\n",
    "# # Iterate over the training data loader to count labels\n",
    "# for _, labels in train_data_loader:\n",
    "#     for label in labels:\n",
    "#         label_count[label.item()] += 1\n",
    "\n",
    "# # Adding counts from the validation data loader\n",
    "# for _, labels in validation_data_loader:\n",
    "#     for label in labels:\n",
    "#         label_count[label.item()] += 1\n",
    "\n",
    "# # Class names in FashionMNIST (for better visualization)\n",
    "# classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(classes, label_count, color='skyblue')\n",
    "# plt.xlabel('Classes')\n",
    "# plt.ylabel('Number of samples')\n",
    "# plt.title('Distribution of classes in FashionMNIST Dataset')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"--wandb\", action=argparse.BooleanOptionalAction, default=True, help=\"True or False\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-b\", \"--batch_size\", type=int, default=2048, help=\"Batch size (int, default: 2_048)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-e\", \"--epochs\", type=int, default=10_000, help=\"Number of training epochs (int, default:10_000)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-r\", \"--learning_rate\", type=float, default=1e-3, help=\"Learning rate (float, default: 1e-3)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-v\", \"--validation_intervals\", type=int, default=10,\n",
    "    help=\"Number of training epochs between validations (int, default: 10)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-p\", \"--early_stop_patience\", type=int, default=1000,\n",
    "    help=\"Number of early stop patience (int, default: 10)\"\n",
    "  )\n",
    "\n",
    "  return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from _01_code._99_common_utils.utils import strfdelta\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.0001, project_name=None, checkpoint_file_path=None, run_time_str=None):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.delta = delta\n",
    "        self.val_loss_min = None\n",
    "        self.file_path = os.path.join(\n",
    "            checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "        )\n",
    "        self.latest_file_path = os.path.join(\n",
    "            checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "        )\n",
    "\n",
    "    def check_and_save(self, new_validation_loss, model):\n",
    "        early_stop = False\n",
    "\n",
    "        if self.val_loss_min is None:\n",
    "            self.val_loss_min = new_validation_loss\n",
    "            message = f'Early stopping is stated!'\n",
    "        elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "            message = f'V_loss decreased ({self.val_loss_min:6.3f} --> {new_validation_loss:6.3f}). Saving model...'\n",
    "            self.save_checkpoint(new_validation_loss, model)\n",
    "            self.val_loss_min = new_validation_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "            if self.counter >= self.patience:\n",
    "                early_stop = True\n",
    "                message += \" *** TRAIN EARLY STOPPED! ***\"\n",
    "        return message, early_stop\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        torch.save(model.state_dict(), self.file_path)\n",
    "        torch.save(model.state_dict(), self.latest_file_path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class ClassificationTrainer:\n",
    "    def __init__(\n",
    "        self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "        run_time_str, wandb, device, checkpoint_file_path\n",
    "    ):\n",
    "        self.project_name = project_name\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.validation_data_loader = validation_data_loader\n",
    "        self.transforms = transforms\n",
    "        self.run_time_str = run_time_str\n",
    "        self.wandb = wandb\n",
    "        self.device = device\n",
    "        self.checkpoint_file_path = checkpoint_file_path\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def do_train(self):\n",
    "        self.model.train()\n",
    "        loss_train = 0.0\n",
    "        num_corrects_train = 0\n",
    "        num_trained_samples = 0\n",
    "        num_trains = 0\n",
    "\n",
    "        for train_batch in self.train_data_loader:\n",
    "            input_train, target_train = train_batch\n",
    "            input_train = input_train.to(device=self.device)\n",
    "            target_train = target_train.to(device=self.device)\n",
    "\n",
    "            if self.transforms:\n",
    "                input_train = self.transforms(input_train)\n",
    "\n",
    "            output_train = self.model(input_train)\n",
    "\n",
    "            loss = self.loss_fn(output_train, target_train)\n",
    "            loss_train += loss.item()\n",
    "\n",
    "            predicted_train = torch.argmax(output_train, dim=1)\n",
    "            num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "            num_trained_samples += len(input_train)\n",
    "            num_trains += 1\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        train_loss = loss_train / num_trains\n",
    "        train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "    def do_validation(self):\n",
    "        self.model.eval()\n",
    "        loss_validation = 0.0\n",
    "        num_corrects_validation = 0\n",
    "        num_validated_samples = 0\n",
    "        num_validations = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for validation_batch in self.validation_data_loader:\n",
    "                input_validation, target_validation = validation_batch\n",
    "                input_validation = input_validation.to(device=self.device)\n",
    "                target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "                if self.transforms:\n",
    "                    input_validation = self.transforms(input_validation)\n",
    "\n",
    "                output_validation = self.model(input_validation)\n",
    "                loss = self.loss_fn(output_validation, target_validation)\n",
    "                loss_validation += loss.item()\n",
    "\n",
    "                predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "                num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "                num_validated_samples += len(input_validation)\n",
    "                num_validations += 1\n",
    "\n",
    "        validation_loss = loss_validation / num_validations\n",
    "        validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "        return validation_loss, validation_accuracy\n",
    "\n",
    "    def train_loop(self):\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=self.wandb.config.early_stop_patience,\n",
    "            delta=self.wandb.config.early_stop_patience,\n",
    "            project_name=self.project_name,\n",
    "            checkpoint_file_path=self.checkpoint_file_path,\n",
    "            run_time_str=self.run_time_str\n",
    "        )\n",
    "        n_epochs = self.wandb.config.epochs\n",
    "        training_start_time = datetime.now()\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss, train_accuracy = self.do_train()\n",
    "            validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "            elapsed_time = datetime.now() - training_start_time\n",
    "            epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "            message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "            print(\n",
    "                f\"[Epoch {epoch:>3}] \"\n",
    "                f\"T_loss: {train_loss:6.4f}, \"\n",
    "                f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "                f\"V_loss: {validation_loss:6.4f}, \"\n",
    "                f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "                f\"{message} | \"\n",
    "                f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "                f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "            )\n",
    "\n",
    "            self.wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Training loss\": train_loss,\n",
    "                \"Training accuracy (%)\": train_accuracy,\n",
    "                \"Validation loss\": validation_loss,\n",
    "                \"Validation accuracy (%)\": validation_accuracy,\n",
    "                \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "            })\n",
    "\n",
    "            if early_stop:\n",
    "                break\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAO73JcbeM-p"
   },
   "source": [
    "# My Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(2),\n",
    "        transforms.RandomAffine(degrees = 0, shear = 0.2, scale = (0.8,1.2),translate=(0.2,0.2))\n",
    "    ])\n",
    "\n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "    \n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "    \n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "    \n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_data_loading_workers,\n",
    "        drop_last = True\n",
    "    )\n",
    "    \n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_data_loading_workers,\n",
    "        drop_last = True\n",
    "    )\n",
    "    \n",
    "    imgs = torch.stack([img_t for img_t, _ in train_data_loader], dim = 3)\n",
    "    x, y = get_mean_std(imgs)\n",
    "    \n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=x, std=y),\n",
    "    )\n",
    "    \n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 32\n"
     ]
    }
   ],
   "source": [
    "config = {'batch_size': 2048,}\n",
    "wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "my_train_data_loader, my_validation_data_loader, my_f_mnist_transforms = get_fashion_mnist_data()\n",
    "print()\n",
    "my_f_mnist_test_images, my_test_data_loader, my_f_mnist_transforms = get_fashion_mnist_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_model():\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(MyModel, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm2d(32)\n",
    "            self.conv3 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "            self.bn3 = nn.BatchNorm2d(32)\n",
    "            self.pool = nn.MaxPool2d(3, 3)\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Linear(32 * 9 * 9, 64)  # 계산된 입력 차원\n",
    "            self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.bn1(self.conv1(x)))\n",
    "            x1 = torch.relu(self.bn2(self.conv2(x)))\n",
    "            x2 = torch.relu(self.bn3(self.conv3(x1)))\n",
    "\n",
    "            # Attention 적용\n",
    "            att = torch.bmm(x1.view(x1.size(0), x1.size(1), -1).transpose(1, 2), x2.view(x2.size(0), x2.size(1), -1))\n",
    "            att = softmax(att, dim=2)\n",
    "            x_att = torch.bmm(x2.view(x2.size(0), x2.size(1), -1), att.transpose(1, 2)).view_as(x2)\n",
    "\n",
    "            x = self.pool(x_att)\n",
    "            x = self.flatten(x)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "    \n",
    "    my_model = MyModel(10)\n",
    "    \n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    \n",
    "    config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'validation_intervals': args.validation_intervals,\n",
    "    'learning_rate': args.learning_rate,\n",
    "    'early_stop_patience': args.early_stop_patience\n",
    "    }\n",
    "    \n",
    "    project_name = 'Fashion_MNIST'\n",
    "    wandb.init(\n",
    "        mode = \"online\" if args.wandb else \"disabled\",\n",
    "        project = project_name,\n",
    "        notes = \"Assignment #3 with Fashion MNIST dataset\",\n",
    "        tags = [\"ComputerVision\", \"ImageClassification\",\"CNN\"],\n",
    "        name = run_time_str,\n",
    "        config = config\n",
    "    )\n",
    "    \n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "    print(f\"Training on Device {device}\")\n",
    "    \n",
    "    train_data_loader, validation_data_loader, mnist_transforms = get_fashion_mnist_data()\n",
    "    model = get_my_model()\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr = wandb.config.learning_rate)\n",
    "    \n",
    "    classification_trainer = ClassificationTrainer(\n",
    "        project_name, model, optimizer, train_data_loader, validation_data_loader, mnist_transforms,\n",
    "        run_time_str, wandb, device, checkpoint_file_path=\"C:/Users/nazero/Desktop/link_dl-main/link_dl-main/_02_homeworks/03_fashion_mnist/checkpoints\"\n",
    "    )\n",
    "    \n",
    "    classification_trainer.train_loop()\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i3t5av2h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2023-11-10_09-53-43</strong> at: <a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/i3t5av2h' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST/runs/i3t5av2h</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231110_095343-i3t5av2h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i3t5av2h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73502a6330984b49b6200a18ded5b28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888889738, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\nazero\\Desktop\\link_dl-main\\link_dl-main\\_02_homeworks\\_03_fashion_mnist\\wandb\\run-20231110_095656-9h3263pw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/9h3263pw' target=\"_blank\">2023-11-10_09-56-56</a></strong> to <a href='https://wandb.ai/ydj9805/Fashion_MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ydj9805/Fashion_MNIST' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/9h3263pw' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST/runs/9h3263pw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(wandb=True, batch_size=2048, epochs=10000, learning_rate=0.001, validation_intervals=10, early_stop_patience=10)\n",
      "{'epochs': 10000, 'batch_size': 2048, 'validation_intervals': 10, 'learning_rate': 0.001, 'early_stop_patience': 10}\n",
      "Training on Device cuda:0\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 32\n",
      "[Epoch   1] T_loss: 2.2483, T_accuracy: 19.4787 | V_loss: 2.2595, V_accuracy: 22.9492 | Early stopping is stated! | T_time: 00:02:12, T_speed: 0.008\n",
      "[Epoch   2] T_loss: 2.0525, T_accuracy: 36.6248 | V_loss: 1.9826, V_accuracy: 41.2354 | Early stopping counter: 1 out of 10 | T_time: 00:04:21, T_speed: 0.008\n",
      "[Epoch   3] T_loss: 1.8616, T_accuracy: 45.7520 | V_loss: 1.7710, V_accuracy: 48.8770 | Early stopping counter: 2 out of 10 | T_time: 00:06:31, T_speed: 0.008\n",
      "[Epoch   4] T_loss: 1.6752, T_accuracy: 51.2639 | V_loss: 1.5915, V_accuracy: 53.8086 | Early stopping counter: 3 out of 10 | T_time: 00:08:40, T_speed: 0.008\n",
      "[Epoch   5] T_loss: 1.5077, T_accuracy: 55.2622 | V_loss: 1.4296, V_accuracy: 57.7881 | Early stopping counter: 4 out of 10 | T_time: 00:10:50, T_speed: 0.008\n",
      "[Epoch   6] T_loss: 1.3578, T_accuracy: 59.4313 | V_loss: 1.2858, V_accuracy: 61.5723 | Early stopping counter: 5 out of 10 | T_time: 00:12:59, T_speed: 0.008\n",
      "[Epoch   7] T_loss: 1.2272, T_accuracy: 63.0690 | V_loss: 1.1646, V_accuracy: 64.6729 | Early stopping counter: 6 out of 10 | T_time: 00:15:08, T_speed: 0.008\n",
      "[Epoch   8] T_loss: 1.1180, T_accuracy: 65.7752 | V_loss: 1.0667, V_accuracy: 67.2607 | Early stopping counter: 7 out of 10 | T_time: 00:17:17, T_speed: 0.008\n",
      "[Epoch   9] T_loss: 1.0298, T_accuracy: 67.6946 | V_loss: 0.9875, V_accuracy: 68.7500 | Early stopping counter: 8 out of 10 | T_time: 00:19:26, T_speed: 0.008\n",
      "[Epoch  10] T_loss: 0.9589, T_accuracy: 69.1857 | V_loss: 0.9255, V_accuracy: 70.0684 | Early stopping counter: 9 out of 10 | T_time: 00:21:36, T_speed: 0.008\n",
      "[Epoch  11] T_loss: 0.9046, T_accuracy: 70.2994 | V_loss: 0.8763, V_accuracy: 70.9717 | Early stopping counter: 10 out of 10 *** TRAIN EARLY STOPPED! *** | T_time: 00:23:45, T_speed: 0.008\n",
      "Final training time: 00:23:45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.030 MB uploaded\\r'), FloatProgress(value=0.039566833683529985, max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>Training accuracy (%)</td><td>▁▃▅▅▆▇▇▇███</td></tr><tr><td>Training loss</td><td>█▇▆▅▄▃▃▂▂▁▁</td></tr><tr><td>Training speed (epochs/sec.)</td><td>▁▅▆▇▇▇█████</td></tr><tr><td>Validation accuracy (%)</td><td>▁▄▅▅▆▇▇▇███</td></tr><tr><td>Validation loss</td><td>█▇▆▅▄▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>11</td></tr><tr><td>Training accuracy (%)</td><td>70.29935</td></tr><tr><td>Training loss</td><td>0.90456</td></tr><tr><td>Training speed (epochs/sec.)</td><td>0.00772</td></tr><tr><td>Validation accuracy (%)</td><td>70.97168</td></tr><tr><td>Validation loss</td><td>0.87634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2023-11-10_09-56-56</strong> at: <a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/9h3263pw' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST/runs/9h3263pw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231110_095656-9h3263pw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser = get_parser()\n",
    "args,_ = parser.parse_known_args()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
