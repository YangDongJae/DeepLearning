{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jqqybZBF_sfq"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "BASE_PATH = \"/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/\"\n",
    "sys.path.append(BASE_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _01_code._99_common_utils.utils import get_num_cpu_cores, is_linux, is_windows\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.nn.functional import softmax\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from datetime import datetime\n",
    "import os\n",
    "from _01_code._99_common_utils.utils import strfdelta\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mydj9805\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "  def __init__(self, patience=10, delta=0.0001, project_name=None, checkpoint_file_path=None, run_time_str=None):\n",
    "    self.patience = patience\n",
    "    self.counter = 0\n",
    "    self.delta = delta\n",
    "\n",
    "    self.val_loss_min = None\n",
    "    self.file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "    )\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "  def check_and_save(self, new_validation_loss, model):\n",
    "    early_stop = False\n",
    "\n",
    "    if self.val_loss_min is None:\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      message = f'Early stopping is stated!'\n",
    "    elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "      message = f'V_loss decreased ({self.val_loss_min:6.3f} --> {new_validation_loss:6.3f}). Saving model...'\n",
    "      self.save_checkpoint(new_validation_loss, model)\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      self.counter = 0\n",
    "    else:\n",
    "      self.counter += 1\n",
    "      message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "      if self.counter >= self.patience:\n",
    "        early_stop = True\n",
    "        message += \" *** TRAIN EARLY STOPPED! ***\"\n",
    "\n",
    "    return message, early_stop\n",
    "\n",
    "  def save_checkpoint(self, val_loss, model):\n",
    "    '''Saves model when validation loss decrease.'''\n",
    "    torch.save(model.state_dict(), self.file_path)\n",
    "    torch.save(model.state_dict(), self.latest_file_path)\n",
    "    self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path, scheduler = None\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "    self.scheduler = scheduler\n",
    "\n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()\n",
    "\n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:6.4f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:6.4f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break\n",
    "        if self.scheduler:\n",
    "          self.scheduler.step()        \n",
    "\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")\n",
    "    \n",
    "    \n",
    "class ClassificationTester:\n",
    "  def __init__(self, project_name, model, test_data_loader, transforms, checkpoint_file_path):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.test_data_loader = test_data_loader\n",
    "    self.transforms = transforms\n",
    "\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "    print(\"MODEL FILE: {0}\".format(self.latest_file_path))\n",
    "\n",
    "    self.model.load_state_dict(torch.load(self.latest_file_path, map_location=torch.device('cpu')))\n",
    "\n",
    "  def test(self):\n",
    "    self.model.eval()    # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    num_corrects_test = 0\n",
    "    num_tested_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for test_batch in self.test_data_loader:\n",
    "        input_test, target_test = test_batch\n",
    "\n",
    "        input_test = self.transforms(input_test)\n",
    "\n",
    "        output_test = self.model(input_test)\n",
    "\n",
    "        predicted_test = torch.argmax(output_test, dim=1)\n",
    "        num_corrects_test += torch.sum(torch.eq(predicted_test, target_test))\n",
    "\n",
    "        num_tested_samples += len(input_test)\n",
    "\n",
    "      test_accuracy = 100.0 * num_corrects_test / num_tested_samples\n",
    "\n",
    "    print(f\"TEST RESULTS: {test_accuracy:6.3f}%\")\n",
    "\n",
    "  def test_single(self, input_test):\n",
    "    self.model.eval()    # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    with torch.no_grad():\n",
    "      input_test = self.transforms(input_test)\n",
    "\n",
    "      output_test = self.model(input_test)\n",
    "      predicted_test = torch.argmax(output_test, dim=1)\n",
    "\n",
    "    return predicted_test.item()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"--wandb\", action=argparse.BooleanOptionalAction, default=True, help=\"True or False\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-b\", \"--batch_size\", type=int, default=2_048, help=\"Batch size (int, default: 2_048)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-e\", \"--epochs\", type=int, default=2_500, help=\"Number of training epochs (int, default:10_000)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-r\", \"--learning_rate\", type=float, default=1e-3, help=\"Learning rate (float, default: 1e-3)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-v\", \"--validation_intervals\", type=int, default=10,\n",
    "    help=\"Number of training epochs between validations (int, default: 10)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-p\", \"--early_stop_patience\", type=int, default=10,\n",
    "    help=\"Number of early stop patience (int, default: 10)\"\n",
    "  )\n",
    "\n",
    "  return parser\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture's Approach to get Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AAyFE8IqAo6N"
   },
   "outputs": [],
   "source": [
    "def get_mean_std(imgs):\n",
    "  mean = imgs.view(1, -1).mean(dim = -1)\n",
    "  std = imgs.view(1, -1).std(dim = -1)\n",
    "\n",
    "  return mean, std\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "  data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "  f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "  f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "  print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "  print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "  print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "  num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "  print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=f_mnist_train,\n",
    "     batch_size=wandb.config.batch_size,\n",
    "     shuffle=True,\n",
    "    pin_memory=True,\n",
    "     num_workers=num_data_loading_workers,\n",
    "     drop_last = True\n",
    "  )\n",
    "\n",
    "  validation_data_loader = DataLoader(\n",
    "    dataset=f_mnist_validation,\n",
    "     batch_size=wandb.config.batch_size,\n",
    "     pin_memory=True,\n",
    "     num_workers=num_data_loading_workers,\n",
    "     drop_last = True\n",
    "  )\n",
    "\n",
    "  imgs = torch.stack([img_t for img_t, _ in train_data_loader], dim = 3)\n",
    "  x, y = get_mean_std(imgs)\n",
    "\n",
    "  f_mnist_transforms = nn.Sequential(\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(mean=x, std=y),\n",
    "  )\n",
    "\n",
    "  return train_data_loader, validation_data_loader, f_mnist_transforms\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SAO73JcbeM-p"
   },
   "source": [
    "## My Approach to get Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "    # Step 1: Load the dataset without any normalization to calculate mean and std\n",
    "    temp_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    temp_dataset = datasets.FashionMNIST(data_path, train=True, download=True, transform=temp_transform)\n",
    "\n",
    "    # Calculate mean and std\n",
    "    data_loader_for_calc = DataLoader(temp_dataset, batch_size=len(temp_dataset), shuffle=False)\n",
    "    data = next(iter(data_loader_for_calc))[0]\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "\n",
    "    # Step 2: Define the transformations with calculated mean and std\n",
    "    my_f_mnist_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(2.0),\n",
    "        transforms.RandomAffine(degrees=0, shear=0.2),\n",
    "        transforms.RandomResizedCrop((28, 28), scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "\n",
    "    # Now apply these transformations when creating the dataset\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=my_f_mnist_transforms)\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "\n",
    "    # Create DataLoaders for training and validation datasets\n",
    "    my_train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_data_loading_workers,\n",
    "        drop_last=True)\n",
    "\n",
    "    my_validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_data_loading_workers,\n",
    "        drop_last=True)\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    return my_train_data_loader, my_validation_data_loader, my_f_mnist_transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_test_data():\n",
    "  data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "  f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "  f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "  print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "  print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "  test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "  imgs = torch.stack([img_t for img_t, _ in test_data_loader], dim = 3)\n",
    "  x, y = get_mean_std(imgs)\n",
    "\n",
    "  f_mnist_transforms = nn.Sequential(\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(mean=x, std=y),\n",
    "  )\n",
    "\n",
    "  return f_mnist_test_images, test_data_loader, f_mnist_transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 0\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "config = {'batch_size': 2048,}\n",
    "wandb.init(mode=\"disabled\", config=config)\n",
    "\n",
    "my_train_data_loader, my_validation_data_loader, my_f_mnist_transforms = my_get_fashion_mnist_data()\n",
    "print()\n",
    "my_f_mnist_test_images, my_test_data_loader, my_f_mnist_transforms = get_fashion_mnist_test_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Model(4Conv+2FC+pooling+dropout+BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_model():\n",
    "    class CNNModel5(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNNModel5, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding='same')\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
    "            self.bn2 = nn.BatchNorm2d(128)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "            self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "            self.dropout3 = nn.Dropout(0.5)\n",
    "\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Linear(256 * 7 * 7, 256)  # Adjusted the size\n",
    "            self.bn3 = nn.BatchNorm1d(256)\n",
    "            self.dropout4 = nn.Dropout(0.5)\n",
    "            self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = self.pool1(x)\n",
    "            x = self.dropout1(x)\n",
    "\n",
    "            x = F.relu(self.bn2(self.conv3(x)))\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "            x = F.relu(self.conv4(x))\n",
    "            x = self.pool2(x)\n",
    "            x = self.dropout3(x)\n",
    "\n",
    "            x = self.flatten(x)\n",
    "            x = F.relu(self.bn3(self.fc1(x)))\n",
    "            x = self.dropout4(x)\n",
    "            x = F.log_softmax(self.fc2(x), dim=1)\n",
    "            return x\n",
    "\n",
    "    my_model = CNNModel5()\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    run_time_str += '-4Conv+2FC+pooling+dropout+BN'\n",
    "    config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'validation_intervals': args.validation_intervals,\n",
    "    'learning_rate': args.learning_rate,\n",
    "    'early_stop_patience': args.early_stop_patience\n",
    "    }\n",
    "\n",
    "    project_name = 'Fashion_MNIST'\n",
    "    wandb.init(\n",
    "        mode = \"online\" if args.wandb else \"disabled\",\n",
    "        project = project_name,\n",
    "        notes = \"Assignment #3 with Fashion MNIST dataset\",\n",
    "        tags = [\"ComputerVision\", \"ImageClassification\",\"CNN\"],\n",
    "        name = run_time_str,\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "    print(f\"Training on Device {device}\")\n",
    "\n",
    "    train_data_loader, validation_data_loader, mnist_transforms = get_fashion_mnist_data()\n",
    "    model = get_my_model()\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Example: Decay LR every 30 epochs\n",
    "\n",
    "    classification_trainer = ClassificationTrainer(\n",
    "        project_name, model, optimizer, train_data_loader, validation_data_loader, mnist_transforms,\n",
    "        run_time_str, wandb, device, checkpoint_file_path='/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/4Conv+2FC+pooling+dropout+BN'\n",
    "    )\n",
    "\n",
    "    classification_trainer.train_loop()\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 28, 28]             640\n",
      "       BatchNorm2d-2           [-1, 64, 28, 28]             128\n",
      "            Conv2d-3           [-1, 64, 28, 28]          36,928\n",
      "         MaxPool2d-4           [-1, 64, 14, 14]               0\n",
      "           Dropout-5           [-1, 64, 14, 14]               0\n",
      "            Conv2d-6          [-1, 128, 14, 14]          73,856\n",
      "       BatchNorm2d-7          [-1, 128, 14, 14]             256\n",
      "           Dropout-8          [-1, 128, 14, 14]               0\n",
      "            Conv2d-9          [-1, 256, 14, 14]         295,168\n",
      "        MaxPool2d-10            [-1, 256, 7, 7]               0\n",
      "          Dropout-11            [-1, 256, 7, 7]               0\n",
      "          Flatten-12                [-1, 12544]               0\n",
      "           Linear-13                  [-1, 256]       3,211,520\n",
      "      BatchNorm1d-14                  [-1, 256]             512\n",
      "          Dropout-15                  [-1, 256]               0\n",
      "           Linear-16                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 3,621,578\n",
      "Trainable params: 3,621,578\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.59\n",
      "Params size (MB): 13.82\n",
      "Estimated Total Size (MB): 16.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_my_model()\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel5(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=12544, out_features=256, bias=True)\n",
       "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout4): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "input_tensor = torch.tensor(np.array(my_f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0).float()\n",
    "dot = make_dot(model(input_tensor), params=dict(model.named_parameters()))\n",
    "dot.render(\"4Conv+2FC+pooling+dropout+BN\", format=\"png\")\n",
    "model.train()  # Set it back to training mode if needed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/4Conv+2FC+pooling+dropout+BN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8zqui73v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17af6b7acc6547d5afcc8d7b0ba679ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167531022187581, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/wandb/run-20231114_230232-xhsfau2o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/xhsfau2o' target=\"_blank\">2023-11-14_23-02-32-4Conv+2FC+pooling+dropout+BN</a></strong> to <a href='https://wandb.ai/ydj9805/Fashion_MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ydj9805/Fashion_MNIST' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/xhsfau2o' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST/runs/xhsfau2o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(wandb=True, batch_size=2048, epochs=2500, learning_rate=0.001, validation_intervals=10, early_stop_patience=10)\n",
      "{'epochs': 2500, 'batch_size': 2048, 'validation_intervals': 10, 'learning_rate': 0.001, 'early_stop_patience': 10}\n",
      "Training on Device mps\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 0\n"
     ]
    }
   ],
   "source": [
    "parser = get_parser()\n",
    "args,_ = parser.parse_known_args()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FILE: /Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/4Conv_2FC_pooling_dropout_BN/Fashion_MNIST_checkpoint_latest.pt\n",
      "TEST RESULTS: 94.280%\n",
      "\n",
      "     LABEL: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION: 9\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_FILE_PATH ='/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/4Conv_2FC_pooling_dropout_BN/'\n",
    "\n",
    "test_model = get_my_model()\n",
    "classification_tester = ClassificationTester(\n",
    "    'Fashion_MNIST', test_model, my_test_data_loader, my_f_mnist_transforms, CHECKPOINT_FILE_PATH\n",
    ")\n",
    "classification_tester.test()\n",
    "\n",
    "print()\n",
    "\n",
    "img, label = my_f_mnist_test_images[0]\n",
    "print(\"     LABEL:\", label)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# torch.tensor(np.array(mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0).shape: (1, 1, 28, 28)\n",
    "output = classification_tester.test_single(\n",
    "torch.tensor(np.array(my_f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    ")\n",
    "print(\"PREDICTION:\", output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Conv+1FC+pooling+l2+BN&learningdecay Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_model():\n",
    "    class CNNModel4(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNNModel4, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding='same')\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
    "            self.bn2 = nn.BatchNorm2d(128)\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Linear(128 * 14 * 14, 128)\n",
    "            self.bn3 = nn.BatchNorm1d(128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = F.relu(self.bn2(self.conv3(x)))\n",
    "            x = self.flatten(x)\n",
    "            x = F.relu(self.bn3(self.fc1(x)))\n",
    "            x = F.softmax(self.fc2(x), dim=1)\n",
    "            return x\n",
    "\n",
    "    my_model = CNNModel4()\n",
    "\n",
    "    return my_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    run_time_str += '-3Conv+1FC+pooling+l2+BN&learningdecay'\n",
    "    config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'validation_intervals': args.validation_intervals,\n",
    "    'learning_rate': args.learning_rate,\n",
    "    'early_stop_patience': args.early_stop_patience\n",
    "    }\n",
    "\n",
    "    project_name = 'Fashion_MNIST'\n",
    "    wandb.init(\n",
    "        mode = \"online\" if args.wandb else \"disabled\",\n",
    "        project = project_name,\n",
    "        notes = \"Assignment #3 with Fashion MNIST dataset\",\n",
    "        tags = [\"ComputerVision\", \"ImageClassification\",\"CNN\"],\n",
    "        name = run_time_str,\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "    print(f\"Training on Device {device}\")\n",
    "\n",
    "    train_data_loader, validation_data_loader, mnist_transforms = get_fashion_mnist_data()\n",
    "    model = get_my_model()\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Example: Decay LR every 30 epochs\n",
    "\n",
    "    classification_trainer = ClassificationTrainer(\n",
    "        project_name, model, optimizer, train_data_loader, validation_data_loader, mnist_transforms,\n",
    "        run_time_str, wandb, device, checkpoint_file_path='/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/Conv+1FC+pooling+l2+BN&learningdecay',scheduler= scheduler\n",
    "    )\n",
    "\n",
    "    classification_trainer.train_loop()\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 28, 28]             640\n",
      "       BatchNorm2d-2           [-1, 64, 28, 28]             128\n",
      "            Conv2d-3           [-1, 64, 28, 28]          36,928\n",
      "         MaxPool2d-4           [-1, 64, 14, 14]               0\n",
      "            Conv2d-5          [-1, 128, 14, 14]          73,856\n",
      "       BatchNorm2d-6          [-1, 128, 14, 14]             256\n",
      "           Flatten-7                [-1, 25088]               0\n",
      "            Linear-8                  [-1, 128]       3,211,392\n",
      "       BatchNorm1d-9                  [-1, 128]             256\n",
      "           Linear-10                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 3,324,746\n",
      "Trainable params: 3,324,746\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.82\n",
      "Params size (MB): 12.68\n",
      "Estimated Total Size (MB): 14.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_my_model()\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "input_tensor = torch.tensor(np.array(my_f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0).float()\n",
    "dot = make_dot(model(input_tensor), params=dict(model.named_parameters()))\n",
    "dot.render(\"3Conv+1FC+pooling+l2+BN&learningdecay\", format=\"png\")\n",
    "model.train()  # Set it back to training mode if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./3Conv+1FC+pooling+l2+BN&learningdecay.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mydj9805\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c020c2712f7443ab895437478c114b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011223280088880629, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/wandb/run-20231114_180747-o9tz5ddn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/o9tz5ddn' target=\"_blank\">2023-11-14_18-07-47-3Conv+1FC+pooling+l2+BN&learningdecay</a></strong> to <a href='https://wandb.ai/ydj9805/Fashion_MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ydj9805/Fashion_MNIST' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/o9tz5ddn' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST/runs/o9tz5ddn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(wandb=True, batch_size=2048, epochs=2500, learning_rate=0.001, validation_intervals=10, early_stop_patience=10)\n",
      "{'epochs': 2500, 'batch_size': 2048, 'validation_intervals': 10, 'learning_rate': 0.001, 'early_stop_patience': 10}\n",
      "Training on Device mps\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 0\n",
      "[Epoch   1] T_loss: 1.8060, T_accuracy: 76.6433 | V_loss: 1.7926, V_accuracy: 85.6934 | Early stopping is stated! | T_time: 00:00:10, T_speed: 0.100\n",
      "[Epoch  10] T_loss: 1.4947, T_accuracy: 97.3915 | V_loss: 1.5420, V_accuracy: 92.7002 | V_loss decreased ( 1.793 -->  1.542). Saving model... | T_time: 00:01:32, T_speed: 0.109\n",
      "[Epoch  20] T_loss: 1.4713, T_accuracy: 99.1530 | V_loss: 1.5339, V_accuracy: 93.1396 | V_loss decreased ( 1.542 -->  1.534). Saving model... | T_time: 00:03:02, T_speed: 0.110\n",
      "[Epoch  30] T_loss: 1.4675, T_accuracy: 99.4122 | V_loss: 1.5317, V_accuracy: 93.1885 | V_loss decreased ( 1.534 -->  1.532). Saving model... | T_time: 00:04:34, T_speed: 0.109\n",
      "[Epoch  40] T_loss: 1.4663, T_accuracy: 99.4967 | V_loss: 1.5305, V_accuracy: 93.3594 | V_loss decreased ( 1.532 -->  1.530). Saving model... | T_time: 00:06:08, T_speed: 0.109\n",
      "[Epoch  50] T_loss: 1.4660, T_accuracy: 99.5286 | V_loss: 1.5300, V_accuracy: 93.4570 | V_loss decreased ( 1.530 -->  1.530). Saving model... | T_time: 00:07:39, T_speed: 0.109\n",
      "[Epoch  60] T_loss: 1.4659, T_accuracy: 99.5267 | V_loss: 1.5301, V_accuracy: 93.2617 | Early stopping counter: 1 out of 10 | T_time: 00:09:10, T_speed: 0.109\n",
      "[Epoch  70] T_loss: 1.4658, T_accuracy: 99.5361 | V_loss: 1.5301, V_accuracy: 93.2373 | Early stopping counter: 2 out of 10 | T_time: 00:10:43, T_speed: 0.109\n",
      "[Epoch  80] T_loss: 1.4656, T_accuracy: 99.5568 | V_loss: 1.5299, V_accuracy: 93.1885 | Early stopping counter: 3 out of 10 | T_time: 00:12:14, T_speed: 0.109\n",
      "[Epoch  90] T_loss: 1.4655, T_accuracy: 99.5681 | V_loss: 1.5297, V_accuracy: 93.2129 | V_loss decreased ( 1.530 -->  1.530). Saving model... | T_time: 00:13:45, T_speed: 0.109\n",
      "[Epoch 100] T_loss: 1.4656, T_accuracy: 99.5624 | V_loss: 1.5297, V_accuracy: 93.3838 | Early stopping counter: 1 out of 10 | T_time: 00:15:18, T_speed: 0.109\n",
      "[Epoch 110] T_loss: 1.4655, T_accuracy: 99.5662 | V_loss: 1.5297, V_accuracy: 93.2861 | Early stopping counter: 2 out of 10 | T_time: 00:16:49, T_speed: 0.109\n",
      "[Epoch 120] T_loss: 1.4655, T_accuracy: 99.5662 | V_loss: 1.5297, V_accuracy: 93.3350 | Early stopping counter: 3 out of 10 | T_time: 00:18:20, T_speed: 0.109\n",
      "[Epoch 130] T_loss: 1.4655, T_accuracy: 99.5662 | V_loss: 1.5297, V_accuracy: 93.2861 | Early stopping counter: 4 out of 10 | T_time: 00:19:51, T_speed: 0.109\n",
      "[Epoch 140] T_loss: 1.4653, T_accuracy: 99.5850 | V_loss: 1.5298, V_accuracy: 93.2861 | Early stopping counter: 5 out of 10 | T_time: 00:21:21, T_speed: 0.109\n",
      "[Epoch 150] T_loss: 1.4655, T_accuracy: 99.5643 | V_loss: 1.5299, V_accuracy: 93.2861 | Early stopping counter: 6 out of 10 | T_time: 00:22:52, T_speed: 0.109\n",
      "[Epoch 160] T_loss: 1.4655, T_accuracy: 99.5681 | V_loss: 1.5299, V_accuracy: 93.2617 | Early stopping counter: 7 out of 10 | T_time: 00:24:23, T_speed: 0.109\n",
      "[Epoch 170] T_loss: 1.4655, T_accuracy: 99.5681 | V_loss: 1.5298, V_accuracy: 93.3350 | Early stopping counter: 8 out of 10 | T_time: 00:25:54, T_speed: 0.109\n",
      "[Epoch 180] T_loss: 1.4654, T_accuracy: 99.5850 | V_loss: 1.5299, V_accuracy: 93.2617 | Early stopping counter: 9 out of 10 | T_time: 00:27:25, T_speed: 0.109\n",
      "[Epoch 190] T_loss: 1.4655, T_accuracy: 99.5718 | V_loss: 1.5299, V_accuracy: 93.2617 | Early stopping counter: 10 out of 10 *** TRAIN EARLY STOPPED! *** | T_time: 00:28:56, T_speed: 0.109\n",
      "Final training time: 00:28:56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e856df4c66474057945783373d156e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>Training accuracy (%)</td><td>â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>Training loss</td><td>â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Training speed (epochs/sec.)</td><td>â–â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>Validation accuracy (%)</td><td>â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>Validation loss</td><td>â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>190</td></tr><tr><td>Training accuracy (%)</td><td>99.57181</td></tr><tr><td>Training loss</td><td>1.46547</td></tr><tr><td>Training speed (epochs/sec.)</td><td>0.10945</td></tr><tr><td>Validation accuracy (%)</td><td>93.26172</td></tr><tr><td>Validation loss</td><td>1.52985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2023-11-14_18-07-47-3Conv+1FC+pooling+l2+BN&learningdecay</strong> at: <a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/o9tz5ddn' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST/runs/o9tz5ddn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231114_180747-o9tz5ddn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser = get_parser()\n",
    "args,_ = parser.parse_known_args()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FILE: /Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/3Conv+1FC+pooling+l2+BN&learningdecay/Fashion_MNIST_checkpoint_latest.pt\n",
      "TEST RESULTS: 93.060%\n",
      "\n",
      "     LABEL: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAee0lEQVR4nO3dfXCU9d3v8c/maQmQbAwhTyVgQAEVSU+ppByVYsnw0BkHlDnHp84Bj4MjDU6RWh06Ktp2Ji3OWEeH6j8t1DOi1hmB0blLbw0m3LaBDgjDcNrmJjQtcEOCUrObB/L8O39wTLtChN/Fbr5JeL9mrhmye333+uaXa/nkym6+CTnnnAAAGGIp1g0AAK5OBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMpFk38EX9/f06deqUsrKyFAqFrNsBAHhyzqm1tVXFxcVKSRn8OmfYBdCpU6dUUlJi3QYA4AqdOHFCkyZNGvT+YRdAWVlZkqTb9G2lKd24GwCAr1716CP928D/54NJWgBt3rxZzz//vJqamlRWVqaXX35Zc+fOvWTd5z92S1O60kIEEACMOP9/wuilXkZJypsQ3nrrLa1fv14bN27Uxx9/rLKyMi1evFhnzpxJxuEAACNQUgLohRde0OrVq/Xggw/qxhtv1KuvvqqxY8fqV7/6VTIOBwAYgRIeQN3d3Tpw4IAqKir+eZCUFFVUVKiuru6C/bu6uhSLxeI2AMDol/AA+vTTT9XX16eCgoK42wsKCtTU1HTB/lVVVYpEIgMb74ADgKuD+S+ibtiwQdFodGA7ceKEdUsAgCGQ8HfB5eXlKTU1Vc3NzXG3Nzc3q7Cw8IL9w+GwwuFwotsAAAxzCb8CysjI0Jw5c1RdXT1wW39/v6qrqzVv3rxEHw4AMEIl5feA1q9fr5UrV+rrX/+65s6dqxdffFHt7e168MEHk3E4AMAIlJQAuueee/TJJ5/omWeeUVNTk7761a9q165dF7wxAQBw9Qo555x1E/8qFospEologZYxCQEARqBe16Ma7VQ0GlV2dvag+5m/Cw4AcHUigAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAiYQH0LPPPqtQKBS3zZw5M9GHAQCMcGnJeNCbbrpJH3zwwT8PkpaUwwAARrCkJENaWpoKCwuT8dAAgFEiKa8BHT16VMXFxZo6daoeeOABHT9+fNB9u7q6FIvF4jYAwOiX8AAqLy/X1q1btWvXLr3yyitqbGzU7bffrtbW1ovuX1VVpUgkMrCVlJQkuiUAwDAUcs65ZB6gpaVFU6ZM0QsvvKCHHnrogvu7urrU1dU18HEsFlNJSYkWaJnSQunJbA0AkAS9rkc12qloNKrs7OxB90v6uwNycnI0ffp0NTQ0XPT+cDiscDic7DYAAMNM0n8PqK2tTceOHVNRUVGyDwUAGEESHkCPP/64amtr9be//U1/+MMfdNdddyk1NVX33Xdfog8FABjBEv4juJMnT+q+++7T2bNnNXHiRN12223au3evJk6cmOhDAQBGsIQH0JtvvpnohwQAjELMggMAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGAi6X+QDgAGE0rz/y/I9fX5Hyi5f/g5TsrYsd41/R0d3jWh/3aTd40kuYP/N1BdMnAFBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwTRs4EqFQgFqAnzv1+8/BTr1+qn+x5F0ZkGBd03+23/yrulriXrXDHdBJlsH8df/mR2orvRgghu5AlwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMEwUsBCgMGiQTRV+A8VlaTPvt7jXdNedJN3zeQf/cG7ZrhLm1LiXfNfy/xr0lu9S4YdroAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYYBgpcIVCaeneNa6n27ump2KOd010hvOukaT0T/w/p65pnf41/36td01TS5Z3zdgx/ustSZ+djHjXpF/T5V0TyfrUuyZ6yr+34YYrIACACQIIAGDCO4D27NmjO++8U8XFxQqFQtqxY0fc/c45PfPMMyoqKlJmZqYqKip09OjRRPULABglvAOovb1dZWVl2rx580Xv37Rpk1566SW9+uqr2rdvn8aNG6fFixers9P/58MAgNHL+00IS5cu1dKlSy96n3NOL774op566iktW7ZMkvTaa6+poKBAO3bs0L333ntl3QIARo2EvgbU2NiopqYmVVRUDNwWiURUXl6uurq6i9Z0dXUpFovFbQCA0S+hAdTU1CRJKiiI/zv0BQUFA/d9UVVVlSKRyMBWUuL/t9EBACOP+bvgNmzYoGg0OrCdOHHCuiUAwBBIaAAVFhZKkpqbm+Nub25uHrjvi8LhsLKzs+M2AMDol9AAKi0tVWFhoaqrqwdui8Vi2rdvn+bNm5fIQwEARjjvd8G1tbWpoaFh4OPGxkYdOnRIubm5mjx5statW6ef/OQnuv7661VaWqqnn35axcXFWr58eSL7BgCMcN4BtH//ft1xxx0DH69fv16StHLlSm3dulVPPPGE2tvb9fDDD6ulpUW33Xabdu3apTFjxiSuawDAiBdyzgWbVpgksVhMkUhEC7RMaSH/gYjAFUlJ9a/p7/MuSc3xHyT555/O8K4JdQX7KXuo379mzORW75r87Dbvmuao/zDSzHCwYaS5Y8951/z1VJ53TSjAl6mvK8C5Kmn6/94fqM5Hr+tRjXYqGo1+6ev65u+CAwBcnQggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJrz/HAOGuVDIvyboQPQgk6NdgDHLAfoLpQU7tV1vb6A6X8e+f6N3TfiM/3FSOwOcD5I6Jvuvw9hwj3fNyU+u8a5JSfU/h/r7g32v/Y+OTP9jdfs/L8JZXd416RnBztUgk9j7WqKBjnUpXAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwTDSoTJUQ0KDDhYNor9vSA4TZLDoUA0VlaQz3/3v3jXd+f6DO3MOp3vX9Ad8hqdld3vX/OOzcd417rMM/5oJ/r2lpwU7V9NTh+YcT0nxf96Oz/QfYCpJPWVTvWtSag8GOtYlHzcpjwoAwCUQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwTDSoTJUQ0JTUr1LQqn+NZLkev0HagZZh6EcLHr6+/6DRVuv8+9vzH/5DxbtyvUukQswA1eSxmT6D/xsOz3e/0Dj/Yd9un7/w7SdC/sXScoM+6+DAs0dDviFCuDvS8Z415TWJqERcQUEADBCAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADAxNU9jDTA4M7AgkxQDAX4/qA/yHBH/5qhlHpdqXfN3+4tCnSsvkz/Yanjj/k/jXrHeZeoL+zfW3dusK9tRrf/5xQKMFAzLTPAQNsA+vqCfa/d2e0/NFZ9/uvQ1eF/nP7+YANMp8w9GaguGbgCAgCYIIAAACa8A2jPnj268847VVxcrFAopB07dsTdv2rVKoVCobhtyZIlieoXADBKeAdQe3u7ysrKtHnz5kH3WbJkiU6fPj2wvfHGG1fUJABg9PF+pXHp0qVaunTpl+4TDodVWFgYuCkAwOiXlNeAampqlJ+frxkzZmjNmjU6e/bsoPt2dXUpFovFbQCA0S/hAbRkyRK99tprqq6u1s9+9jPV1tZq6dKl6uu7+NtBq6qqFIlEBraSkpJEtwQAGIYS/ntA995778C/b775Zs2ePVvTpk1TTU2NFi5ceMH+GzZs0Pr16wc+jsVihBAAXAWS/jbsqVOnKi8vTw0NDRe9PxwOKzs7O24DAIx+SQ+gkydP6uzZsyoqCvab6QCA0cn7R3BtbW1xVzONjY06dOiQcnNzlZubq+eee04rVqxQYWGhjh07pieeeELXXXedFi9enNDGAQAjm3cA7d+/X3fcccfAx5+/frNy5Uq98sorOnz4sH7961+rpaVFxcXFWrRokX784x8rHA4nrmsAwIjnHUALFiyQc4MPRfzd7353RQ19LpSWplDo8ttzvb3+BxnmQzjlhqa/tJJJgerOzSjwrvnHDf7fiJwr9B/CmdLtXSJJSm/1H/DYHfHvrzfLv8al+9coI8AQXEkuwKDLyKSod0043f95+4+o/yTXvt5gg4eDrINSAnxtzwUYaJsa4HyQ9Gmb//pNnFfmtb/r7ZT+uPOS+zELDgBgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIuF/kjtRXG+vXCjAJFoPaddODlR3bnq+d03PeP9pvN3j/L8/6M30LlHrtf41ktSXGWBKdY9/TVq7/3ngAn5r1Z3t31/fGP+aUJDh7Zn+k61D54JNge7p9l/A7gz/T6qlOcu7Jj27y7tmTGaw8ejtLf5PqPRx/seamNPmXRPtCPBkl3RDXrN3zcn86732773M5zlXQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwM22Gkvtr+R7l/TXGwQY0pAQZJdub517jUAEMu+/wHd6b0+h9HkkJt/sfqHed/rM6CPu8aBZ1jm+E/8DO1xf9pFGRYaup4/xMvJcX/85Gkno5075pz7WHvmtSY/3MwPDHAE3AI9bSM8a450+9/QgQdsJqTcc675pTnEOHLHTrMFRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATw3YYaeuKW5SWfvlD/Xr/11nvY7QdneBdI0ljmv1zO73N/zguJcBg0QDzCV1qwMmdAcrSAwww7U/3X+9QsBmc6skKMJg1wDr0jfE/jgvwOYXSgg2azc2PedfcMOGM/4Gu8y/JTu/0rkkLBRhoK0kl/iVNndneNflh//8g/tE91rtGkk51RLxrMk+1e+3f29d1WftxBQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMDEsB1GmvMff1NaSsZl7/+fc6d6HyP/xk+8ayRpyi2fBarz1dmb7l3T3DHeu+bTz7K8aySpt+Xyvz6fS4+letf0pwcY3BlwvqrL7fGu+erU4941E8f4D5+cmvmpd02fC/Y95g/z6r1rfnb2eu+af2++wbvm+envedfkpoa9aySpzwUb5uqrw/mfd7/rmBzoWA2dBd41/5HzFa/9e3svbz+ugAAAJgggAIAJrwCqqqrSLbfcoqysLOXn52v58uWqr4+/VO/s7FRlZaUmTJig8ePHa8WKFWpubk5o0wCAkc8rgGpra1VZWam9e/fq/fffV09PjxYtWqT29n/+saLHHntM7777rt5++23V1tbq1KlTuvvuuxPeOABgZPN6E8KuXbviPt66davy8/N14MABzZ8/X9FoVL/85S+1bds2fetb35IkbdmyRTfccIP27t2rb3zjG4nrHAAwol3Ra0DRaFSSlJubK0k6cOCAenp6VFFRMbDPzJkzNXnyZNXV1V30Mbq6uhSLxeI2AMDoFziA+vv7tW7dOt16662aNWuWJKmpqUkZGRnKycmJ27egoEBNTU0XfZyqqipFIpGBraQkwB9hBwCMOIEDqLKyUkeOHNGbb755RQ1s2LBB0Wh0YDtx4sQVPR4AYGQI9Iuoa9eu1Xvvvac9e/Zo0qRJA7cXFhaqu7tbLS0tcVdBzc3NKiwsvOhjhcNhhcPBfkkMADByeV0BOee0du1abd++Xbt371ZpaWnc/XPmzFF6erqqq6sHbquvr9fx48c1b968xHQMABgVvK6AKisrtW3bNu3cuVNZWVkDr+tEIhFlZmYqEonooYce0vr165Wbm6vs7Gw9+uijmjdvHu+AAwDE8QqgV155RZK0YMGCuNu3bNmiVatWSZJ+/vOfKyUlRStWrFBXV5cWL16sX/ziFwlpFgAweoScG6Jpe5cpFospEologZYpLeQ/jHMopF5zjXdNbOF075rPpvsP7kyb6z8odVqu/5BLSZo8zv9YXwn716TK/xTtU7BppD39/i+L/qmtyLum7q+ll97pC675cIx3zcQ3D3vXSFL/v/xy+XDTX+3/Ttk7Jv5noGMdbvUbwilJTe3Z3jVn28d61/T2+v//IEk93f7n+PTKv3rt3+u6Vd3yfxSNRpWdPfh6MAsOAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCadgAgITqdT2q0U6mYQMAhicCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJrwCqqqrSLbfcoqysLOXn52v58uWqr6+P22fBggUKhUJx2yOPPJLQpgEAI59XANXW1qqyslJ79+7V+++/r56eHi1atEjt7e1x+61evVqnT58e2DZt2pTQpgEAI1+az867du2K+3jr1q3Kz8/XgQMHNH/+/IHbx44dq8LCwsR0CAAYla7oNaBoNCpJys3Njbv99ddfV15enmbNmqUNGzaoo6Nj0Mfo6upSLBaL2wAAo5/XFdC/6u/v17p163Trrbdq1qxZA7fff//9mjJlioqLi3X48GE9+eSTqq+v1zvvvHPRx6mqqtJzzz0XtA0AwAgVcs65IIVr1qzRb3/7W3300UeaNGnSoPvt3r1bCxcuVENDg6ZNm3bB/V1dXerq6hr4OBaLqaSkRAu0TGmh9CCtAQAM9boe1WinotGosrOzB90v0BXQ2rVr9d5772nPnj1fGj6SVF5eLkmDBlA4HFY4HA7SBgBgBPMKIOecHn30UW3fvl01NTUqLS29ZM2hQ4ckSUVFRYEaBACMTl4BVFlZqW3btmnnzp3KyspSU1OTJCkSiSgzM1PHjh3Ttm3b9O1vf1sTJkzQ4cOH9dhjj2n+/PmaPXt2Uj4BAMDI5PUaUCgUuujtW7Zs0apVq3TixAl95zvf0ZEjR9Te3q6SkhLdddddeuqpp77054D/KhaLKRKJ8BoQAIxQSXkN6FJZVVJSotraWp+HBABcpZgFBwAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwkWbdwBc55yRJveqRnHEzAABvveqR9M//zwcz7AKotbVVkvSR/s24EwDAlWhtbVUkEhn0/pC7VEQNsf7+fp06dUpZWVkKhUJx98ViMZWUlOjEiRPKzs426tAe63Ae63Ae63Ae63DecFgH55xaW1tVXFyslJTBX+kZdldAKSkpmjRp0pfuk52dfVWfYJ9jHc5jHc5jHc5jHc6zXocvu/L5HG9CAACYIIAAACZGVACFw2Ft3LhR4XDYuhVTrMN5rMN5rMN5rMN5I2kdht2bEAAAV4cRdQUEABg9CCAAgAkCCABgggACAJgYMQG0efNmXXvttRozZozKy8v1xz/+0bqlIffss88qFArFbTNnzrRuK+n27NmjO++8U8XFxQqFQtqxY0fc/c45PfPMMyoqKlJmZqYqKip09OhRm2aT6FLrsGrVqgvOjyVLltg0myRVVVW65ZZblJWVpfz8fC1fvlz19fVx+3R2dqqyslITJkzQ+PHjtWLFCjU3Nxt1nByXsw4LFiy44Hx45JFHjDq+uBERQG+99ZbWr1+vjRs36uOPP1ZZWZkWL16sM2fOWLc25G666SadPn16YPvoo4+sW0q69vZ2lZWVafPmzRe9f9OmTXrppZf06quvat++fRo3bpwWL16szs7OIe40uS61DpK0ZMmSuPPjjTfeGMIOk6+2tlaVlZXau3ev3n//ffX09GjRokVqb28f2Oexxx7Tu+++q7ffflu1tbU6deqU7r77bsOuE+9y1kGSVq9eHXc+bNq0yajjQbgRYO7cua6ysnLg476+PldcXOyqqqoMuxp6GzdudGVlZdZtmJLktm/fPvBxf3+/KywsdM8///zAbS0tLS4cDrs33njDoMOh8cV1cM65lStXumXLlpn0Y+XMmTNOkqutrXXOnf/ap6enu7fffntgnz//+c9Okqurq7NqM+m+uA7OOffNb37Tfe9737Nr6jIM+yug7u5uHThwQBUVFQO3paSkqKKiQnV1dYad2Th69KiKi4s1depUPfDAAzp+/Lh1S6YaGxvV1NQUd35EIhGVl5dfledHTU2N8vPzNWPGDK1Zs0Znz561bimpotGoJCk3N1eSdODAAfX09MSdDzNnztTkyZNH9fnwxXX43Ouvv668vDzNmjVLGzZsUEdHh0V7gxp2w0i/6NNPP1VfX58KCgribi8oKNBf/vIXo65slJeXa+vWrZoxY4ZOnz6t5557TrfffruOHDmirKws6/ZMNDU1SdJFz4/P77taLFmyRHfffbdKS0t17Ngx/fCHP9TSpUtVV1en1NRU6/YSrr+/X+vWrdOtt96qWbNmSTp/PmRkZCgnJydu39F8PlxsHSTp/vvv15QpU1RcXKzDhw/rySefVH19vd555x3DbuMN+wDCPy1dunTg37Nnz1Z5ebmmTJmi3/zmN3rooYcMO8NwcO+99w78++abb9bs2bM1bdo01dTUaOHChYadJUdlZaWOHDlyVbwO+mUGW4eHH3544N8333yzioqKtHDhQh07dkzTpk0b6jYvatj/CC4vL0+pqakXvIulublZhYWFRl0NDzk5OZo+fboaGhqsWzHz+TnA+XGhqVOnKi8vb1SeH2vXrtV7772nDz/8MO7PtxQWFqq7u1stLS1x+4/W82GwdbiY8vJySRpW58OwD6CMjAzNmTNH1dXVA7f19/erurpa8+bNM+zMXltbm44dO6aioiLrVsyUlpaqsLAw7vyIxWLat2/fVX9+nDx5UmfPnh1V54dzTmvXrtX27du1e/dulZaWxt0/Z84cpaenx50P9fX1On78+Kg6Hy61Dhdz6NAhSRpe54P1uyAux5tvvunC4bDbunWr+9Of/uQefvhhl5OT45qamqxbG1Lf//73XU1NjWtsbHS///3vXUVFhcvLy3Nnzpyxbi2pWltb3cGDB93BgwedJPfCCy+4gwcPur///e/OOed++tOfupycHLdz5053+PBht2zZMldaWurOnTtn3Hlifdk6tLa2uscff9zV1dW5xsZG98EHH7ivfe1r7vrrr3ednZ3WrSfMmjVrXCQScTU1Ne706dMDW0dHx8A+jzzyiJs8ebLbvXu3279/v5s3b56bN2+eYdeJd6l1aGhocD/60Y/c/v37XWNjo9u5c6ebOnWqmz9/vnHn8UZEADnn3Msvv+wmT57sMjIy3Ny5c93evXutWxpy99xzjysqKnIZGRnuK1/5irvnnntcQ0ODdVtJ9+GHHzpJF2wrV650zp1/K/bTTz/tCgoKXDgcdgsXLnT19fW2TSfBl61DR0eHW7RokZs4caJLT093U6ZMcatXrx5136Rd7POX5LZs2TKwz7lz59x3v/tdd80117ixY8e6u+66y50+fdqu6SS41DocP37czZ8/3+Xm5rpwOOyuu+4694Mf/MBFo1Hbxr+AP8cAADAx7F8DAgCMTgQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEz8P5cuoR7uRG/0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION: 9\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_FILE_PATH ='/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/3Conv+1FC+pooling+l2+BN&learningdecay'\n",
    "\n",
    "test_model = get_my_model()\n",
    "classification_tester = ClassificationTester(\n",
    "    'Fashion_MNIST', test_model, my_test_data_loader, my_f_mnist_transforms, CHECKPOINT_FILE_PATH\n",
    ")\n",
    "classification_tester.test()\n",
    "\n",
    "print()\n",
    "\n",
    "img, label = my_f_mnist_test_images[0]\n",
    "print(\"     LABEL:\", label)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# torch.tensor(np.array(mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0).shape: (1, 1, 28, 28)\n",
    "output = classification_tester.test_single(\n",
    "torch.tensor(np.array(my_f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    ")\n",
    "print(\"PREDICTION:\", output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogleNet ADAM Lr Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_model():\n",
    "    class InceptionModule(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(InceptionModule, self).__init__()\n",
    "\n",
    "            # 1x1 convolution\n",
    "            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "            # 3x3 convolution\n",
    "            self.conv3 = nn.Conv2d(in_channels, 128, kernel_size=3, padding=1)\n",
    "            self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "            # 5x5 convolution\n",
    "            self.conv5 = nn.Conv2d(in_channels, 32, kernel_size=5, padding=2)\n",
    "            self.bn5 = nn.BatchNorm2d(32)\n",
    "\n",
    "            # 3x3 max pooling followed by 1x1 convolution\n",
    "            self.pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "            self.conv_pool = nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "            self.bn_pool = nn.BatchNorm2d(32)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "            x3 = F.relu(self.bn3(self.conv3(x)))\n",
    "            x5 = F.relu(self.bn5(self.conv5(x)))\n",
    "            xp = F.relu(self.bn_pool(self.conv_pool(self.pool(x))))\n",
    "\n",
    "            return torch.cat([x1, x3, x5, xp], dim=1)\n",
    "\n",
    "    class MyGoogleNet(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(MyGoogleNet, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.pool1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "            # Inception modules\n",
    "            self.inception1 = InceptionModule(64)\n",
    "            self.inception2 = InceptionModule(256)  # ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(256, num_classes)  # ìµœì¢… ì¶œë ¥ ì°¨ì›ì´ 256ì´ ë  ê²ƒìœ¼ë¡œ ê°€ì •\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.pool1(x)\n",
    "            x = self.inception1(x)\n",
    "            x = self.inception2(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    my_google_net = MyGoogleNet(10)\n",
    "    return my_google_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    run_time_str = run_time_str + 'GOOGLENET_BN_ADAM'\n",
    "\n",
    "    config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'validation_intervals': args.validation_intervals,\n",
    "    'learning_rate': args.learning_rate,\n",
    "    'early_stop_patience': args.early_stop_patience\n",
    "    }\n",
    "\n",
    "    project_name = 'Fashion_MNIST'\n",
    "    wandb.init(\n",
    "        mode = \"online\" if args.wandb else \"disabled\",\n",
    "        project = project_name,\n",
    "        notes = \"Assignment #3 with Fashion MNIST dataset\",\n",
    "        tags = [\"ComputerVision\", \"ImageClassification\",\"CNN\"],\n",
    "        name = run_time_str,\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on Device {device}\")\n",
    "\n",
    "    train_data_loader, validation_data_loader, mnist_transforms = get_fashion_mnist_data()\n",
    "    model = get_my_model()\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Example: Decay LR every 30 epochs\n",
    "\n",
    "    classification_trainer = ClassificationTrainer(\n",
    "        project_name, model, optimizer, train_data_loader, validation_data_loader, mnist_transforms,\n",
    "        run_time_str, wandb, device, checkpoint_file_path='/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/GoogleNetAdam', scheduler=scheduler\n",
    "    )\n",
    "\n",
    "    classification_trainer.train_loop()\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]           3,200\n",
      "       BatchNorm2d-2           [-1, 64, 14, 14]             128\n",
      "         MaxPool2d-3             [-1, 64, 7, 7]               0\n",
      "            Conv2d-4             [-1, 64, 7, 7]           4,160\n",
      "       BatchNorm2d-5             [-1, 64, 7, 7]             128\n",
      "            Conv2d-6            [-1, 128, 7, 7]          73,856\n",
      "       BatchNorm2d-7            [-1, 128, 7, 7]             256\n",
      "            Conv2d-8             [-1, 32, 7, 7]          51,232\n",
      "       BatchNorm2d-9             [-1, 32, 7, 7]              64\n",
      "        MaxPool2d-10             [-1, 64, 7, 7]               0\n",
      "           Conv2d-11             [-1, 32, 7, 7]           2,080\n",
      "      BatchNorm2d-12             [-1, 32, 7, 7]              64\n",
      "  InceptionModule-13            [-1, 256, 7, 7]               0\n",
      "           Conv2d-14             [-1, 64, 7, 7]          16,448\n",
      "      BatchNorm2d-15             [-1, 64, 7, 7]             128\n",
      "           Conv2d-16            [-1, 128, 7, 7]         295,040\n",
      "      BatchNorm2d-17            [-1, 128, 7, 7]             256\n",
      "           Conv2d-18             [-1, 32, 7, 7]         204,832\n",
      "      BatchNorm2d-19             [-1, 32, 7, 7]              64\n",
      "        MaxPool2d-20            [-1, 256, 7, 7]               0\n",
      "           Conv2d-21             [-1, 32, 7, 7]           8,224\n",
      "      BatchNorm2d-22             [-1, 32, 7, 7]              64\n",
      "  InceptionModule-23            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-24            [-1, 256, 1, 1]               0\n",
      "           Linear-25                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 662,794\n",
      "Trainable params: 662,794\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.91\n",
      "Params size (MB): 2.53\n",
      "Estimated Total Size (MB): 3.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_my_model()\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGoogleNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (inception1): InceptionModule(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv_pool): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn_pool): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inception2): InceptionModule(\n",
       "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(256, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv_pool): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn_pool): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "input_tensor = torch.tensor(np.array(my_f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0).float()\n",
    "dot = make_dot(model(input_tensor), params=dict(model.named_parameters()))\n",
    "dot.render(\"GoogleNet_Adam\", format=\"png\")\n",
    "model.train()  # Set it back to training mode if needed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./GoogleNet_Adam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/wandb/run-20231114_230542-5heylbzz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/5heylbzz' target=\"_blank\">2023-11-14_23-05-41GOOGLENET_BN_ADAM</a></strong> to <a href='https://wandb.ai/ydj9805/Fashion_MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ydj9805/Fashion_MNIST' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/5heylbzz' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST/runs/5heylbzz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(wandb=True, batch_size=2048, epochs=2500, learning_rate=0.001, validation_intervals=10, early_stop_patience=10)\n",
      "{'epochs': 2500, 'batch_size': 2048, 'validation_intervals': 10, 'learning_rate': 0.001, 'early_stop_patience': 10}\n",
      "Training on Device cpu\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 0\n",
      "[Epoch   1] T_loss: 1.0613, T_accuracy: 67.9312 | V_loss: 0.7535, V_accuracy: 75.5127 | Early stopping is stated! | T_time: 00:02:29, T_speed: 0.007\n",
      "[Epoch  10] T_loss: 0.2594, T_accuracy: 90.6720 | V_loss: 0.6088, V_accuracy: 80.2490 | V_loss decreased ( 0.754 -->  0.609). Saving model... | T_time: 00:25:17, T_speed: 0.007\n",
      "[Epoch  20] T_loss: 0.1871, T_accuracy: 93.1284 | V_loss: 0.3305, V_accuracy: 88.4521 | V_loss decreased ( 0.609 -->  0.331). Saving model... | T_time: 00:50:35, T_speed: 0.007\n",
      "[Epoch  30] T_loss: 0.1371, T_accuracy: 95.1472 | V_loss: 0.3282, V_accuracy: 88.6230 | V_loss decreased ( 0.331 -->  0.328). Saving model... | T_time: 01:13:51, T_speed: 0.007\n",
      "[Epoch  40] T_loss: 0.0873, T_accuracy: 97.0928 | V_loss: 0.6180, V_accuracy: 84.0820 | Early stopping counter: 1 out of 10 | T_time: 01:36:05, T_speed: 0.007\n",
      "[Epoch  50] T_loss: 0.0560, T_accuracy: 98.3436 | V_loss: 0.5646, V_accuracy: 86.6211 | Early stopping counter: 2 out of 10 | T_time: 01:58:01, T_speed: 0.007\n",
      "[Epoch  60] T_loss: 0.0301, T_accuracy: 99.3427 | V_loss: 0.6748, V_accuracy: 85.0586 | Early stopping counter: 3 out of 10 | T_time: 02:19:57, T_speed: 0.007\n",
      "[Epoch  70] T_loss: 0.0125, T_accuracy: 99.8911 | V_loss: 0.4630, V_accuracy: 88.3545 | Early stopping counter: 4 out of 10 | T_time: 02:42:21, T_speed: 0.007\n",
      "[Epoch  80] T_loss: 0.0201, T_accuracy: 99.5380 | V_loss: 0.8989, V_accuracy: 83.1787 | Early stopping counter: 5 out of 10 | T_time: 03:04:22, T_speed: 0.007\n",
      "[Epoch  90] T_loss: 0.0316, T_accuracy: 98.9802 | V_loss: 2.0231, V_accuracy: 72.9492 | Early stopping counter: 6 out of 10 | T_time: 03:27:18, T_speed: 0.007\n",
      "[Epoch 100] T_loss: 0.0024, T_accuracy: 100.0000 | V_loss: 0.3983, V_accuracy: 90.8691 | Early stopping counter: 7 out of 10 | T_time: 03:49:37, T_speed: 0.007\n",
      "[Epoch 110] T_loss: 0.0013, T_accuracy: 100.0000 | V_loss: 0.4207, V_accuracy: 91.0400 | Early stopping counter: 8 out of 10 | T_time: 04:11:25, T_speed: 0.007\n",
      "[Epoch 120] T_loss: 0.0009, T_accuracy: 100.0000 | V_loss: 0.4385, V_accuracy: 91.2109 | Early stopping counter: 9 out of 10 | T_time: 04:33:11, T_speed: 0.007\n",
      "[Epoch 130] T_loss: 0.0007, T_accuracy: 100.0000 | V_loss: 0.4597, V_accuracy: 90.8447 | Early stopping counter: 10 out of 10 *** TRAIN EARLY STOPPED! *** | T_time: 04:55:16, T_speed: 0.007\n",
      "Final training time: 04:55:16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>â–â–â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ</td></tr><tr><td>Training accuracy (%)</td><td>â–â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>Training loss</td><td>â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Training speed (epochs/sec.)</td><td>â–‚â–â–â–ƒâ–„â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>Validation accuracy (%)</td><td>â–‚â–„â–‡â–‡â–…â–†â–†â–‡â–…â–â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>Validation loss</td><td>â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–ƒâ–ˆâ–â–â–â–‚</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>130</td></tr><tr><td>Training accuracy (%)</td><td>100.0</td></tr><tr><td>Training loss</td><td>0.00069</td></tr><tr><td>Training speed (epochs/sec.)</td><td>0.00734</td></tr><tr><td>Validation accuracy (%)</td><td>90.84473</td></tr><tr><td>Validation loss</td><td>0.4597</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2023-11-14_23-05-41GOOGLENET_BN_ADAM</strong> at: <a href='https://wandb.ai/ydj9805/Fashion_MNIST/runs/5heylbzz' target=\"_blank\">https://wandb.ai/ydj9805/Fashion_MNIST/runs/5heylbzz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231114_230542-5heylbzz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser = get_parser()\n",
    "args,_ = parser.parse_known_args()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FILE: /Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/GoogleNetAdam/Fashion_MNIST_checkpoint_latest.pt\n",
      "TEST RESULTS: 88.780%\n",
      "\n",
      "     LABEL: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION: 9\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_FILE_PATH ='/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/GoogleNetAdam'\n",
    "\n",
    "test_model = get_my_model()\n",
    "classification_tester = ClassificationTester(\n",
    "    'Fashion_MNIST', test_model, my_test_data_loader, my_f_mnist_transforms, CHECKPOINT_FILE_PATH\n",
    ")\n",
    "classification_tester.test()\n",
    "\n",
    "print()\n",
    "\n",
    "img, label = my_f_mnist_test_images[0]\n",
    "print(\"     LABEL:\", label)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# torch.tensor(np.array(mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0).shape: (1, 1, 28, 28)\n",
    "output = classification_tester.test_single(\n",
    "torch.tensor(np.array(my_f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    ")\n",
    "print(\"PREDICTION:\", output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogleNet SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_model():\n",
    "    class InceptionModule(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(InceptionModule, self).__init__()\n",
    "\n",
    "            # 1x1 convolution\n",
    "            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "            # 3x3 convolution\n",
    "            self.conv3 = nn.Conv2d(in_channels, 128, kernel_size=3, padding=1)\n",
    "            self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "            # 5x5 convolution\n",
    "            self.conv5 = nn.Conv2d(in_channels, 32, kernel_size=5, padding=2)\n",
    "            self.bn5 = nn.BatchNorm2d(32)\n",
    "\n",
    "            # 3x3 max pooling followed by 1x1 convolution\n",
    "            self.pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "            self.conv_pool = nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "            self.bn_pool = nn.BatchNorm2d(32)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "            x3 = F.relu(self.bn3(self.conv3(x)))\n",
    "            x5 = F.relu(self.bn5(self.conv5(x)))\n",
    "            xp = F.relu(self.bn_pool(self.conv_pool(self.pool(x))))\n",
    "\n",
    "            return torch.cat([x1, x3, x5, xp], dim=1)\n",
    "\n",
    "    class MyGoogleNet(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(MyGoogleNet, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.pool1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "            # Inception modules\n",
    "            self.inception1 = InceptionModule(64)\n",
    "            self.inception2 = InceptionModule(256)  # ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(256, num_classes)  # ìµœì¢… ì¶œë ¥ ì°¨ì›ì´ 256ì´ ë  ê²ƒìœ¼ë¡œ ê°€ì •\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.pool1(x)\n",
    "            x = self.inception1(x)\n",
    "            x = self.inception2(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    my_google_net = MyGoogleNet(10)\n",
    "    return my_google_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    run_time_str = run_time_str + 'GOOGLENET_BN_ADAM'\n",
    "\n",
    "    config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'validation_intervals': args.validation_intervals,\n",
    "    'learning_rate': args.learning_rate,\n",
    "    'early_stop_patience': args.early_stop_patience\n",
    "    }\n",
    "\n",
    "    project_name = 'Fashion_MNIST'\n",
    "    wandb.init(\n",
    "        mode = \"online\" if args.wandb else \"disabled\",\n",
    "        project = project_name,\n",
    "        notes = \"Assignment #3 with Fashion MNIST dataset\",\n",
    "        tags = [\"ComputerVision\", \"ImageClassification\",\"CNN\"],\n",
    "        name = run_time_str,\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on Device {device}\")\n",
    "\n",
    "    train_data_loader, validation_data_loader, mnist_transforms = get_fashion_mnist_data()\n",
    "    model = get_my_model()\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "    classification_trainer = ClassificationTrainer(\n",
    "        project_name, model, optimizer, train_data_loader, validation_data_loader, mnist_transforms,\n",
    "        run_time_str, wandb, device, checkpoint_file_path='/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/GoogleNetSGD'\n",
    "    )\n",
    "\n",
    "    classification_trainer.train_loop()\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]           3,200\n",
      "       BatchNorm2d-2           [-1, 64, 14, 14]             128\n",
      "         MaxPool2d-3             [-1, 64, 7, 7]               0\n",
      "            Conv2d-4             [-1, 64, 7, 7]           4,160\n",
      "       BatchNorm2d-5             [-1, 64, 7, 7]             128\n",
      "            Conv2d-6            [-1, 128, 7, 7]          73,856\n",
      "       BatchNorm2d-7            [-1, 128, 7, 7]             256\n",
      "            Conv2d-8             [-1, 32, 7, 7]          51,232\n",
      "       BatchNorm2d-9             [-1, 32, 7, 7]              64\n",
      "        MaxPool2d-10             [-1, 64, 7, 7]               0\n",
      "           Conv2d-11             [-1, 32, 7, 7]           2,080\n",
      "      BatchNorm2d-12             [-1, 32, 7, 7]              64\n",
      "  InceptionModule-13            [-1, 256, 7, 7]               0\n",
      "           Conv2d-14             [-1, 64, 7, 7]          16,448\n",
      "      BatchNorm2d-15             [-1, 64, 7, 7]             128\n",
      "           Conv2d-16            [-1, 128, 7, 7]         295,040\n",
      "      BatchNorm2d-17            [-1, 128, 7, 7]             256\n",
      "           Conv2d-18             [-1, 32, 7, 7]         204,832\n",
      "      BatchNorm2d-19             [-1, 32, 7, 7]              64\n",
      "        MaxPool2d-20            [-1, 256, 7, 7]               0\n",
      "           Conv2d-21             [-1, 32, 7, 7]           8,224\n",
      "      BatchNorm2d-22             [-1, 32, 7, 7]              64\n",
      "  InceptionModule-23            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-24            [-1, 256, 1, 1]               0\n",
      "           Linear-25                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 662,794\n",
      "Trainable params: 662,794\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.91\n",
      "Params size (MB): 2.53\n",
      "Estimated Total Size (MB): 3.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_my_model()\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGoogleNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (inception1): InceptionModule(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv_pool): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn_pool): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inception2): InceptionModule(\n",
       "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(256, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv_pool): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn_pool): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "input_tensor = torch.tensor(np.array(my_f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0).float()\n",
    "dot = make_dot(model(input_tensor), params=dict(model.named_parameters()))\n",
    "dot.render(\"GoogleNet_SGD\", format=\"png\")\n",
    "model.train()  # Set it back to training mode if needed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./GoogleNet_SGD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "args,_ = parser.parse_known_args()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FILE: /Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/GoogleNetSGD/Fashion_MNIST_checkpoint_latest.pt\n",
      "TEST RESULTS: 86.460%\n",
      "\n",
      "     LABEL: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION: 9\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_FILE_PATH ='/Users/yangdongjae/Desktop/DeepLearning/link_dl-main/_02_homeworks/_03_fashion_mnist/checkpoints/GoogleNetSGD'\n",
    "\n",
    "test_model = get_my_model()\n",
    "classification_tester = ClassificationTester(\n",
    "    'Fashion_MNIST', test_model, my_test_data_loader, my_f_mnist_transforms, CHECKPOINT_FILE_PATH\n",
    ")\n",
    "classification_tester.test()\n",
    "\n",
    "print()\n",
    "\n",
    "img, label = my_f_mnist_test_images[0]\n",
    "print(\"     LABEL:\", label)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# torch.tensor(np.array(mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0).shape: (1, 1, 28, 28)\n",
    "output = classification_tester.test_single(\n",
    "torch.tensor(np.array(my_f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    ")\n",
    "print(\"PREDICTION:\", output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
